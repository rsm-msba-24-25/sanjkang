[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nSanjit Kangovi\n\n\nMay 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nSanjit Kangovi\n\n\nMay 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(knitr)\n\n# Read the datasets\nairbnb     &lt;- read_csv(\"/Users/sanjitkangovi/Desktop/mysite/blog/project4/airbnb.csv\")\nblueprinty &lt;- read_csv(\"/Users/sanjitkangovi/Desktop/mysite/blog/project4/blueprinty.csv\")\n\n\n\n\n\n\nAirbnb Data\n\n\n\n\n\n\n\n\nFirst 10 rows of Airbnb dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…1\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1\n1\n59\n150\n9\n9\n9\nFALSE\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1\n0\n230\n20\n9\n10\n9\nFALSE\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1\n1\n150\n0\nNA\nNA\nNA\nFALSE\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1\n1\n89\n116\n9\n9\n9\nFALSE\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNA\n1\n39\n93\n9\n8\n9\nTRUE\n\n\n6\n5099\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1\n1\n212\n60\n9\n9\n9\nFALSE\n\n\n7\n5107\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1\n2\n250\n60\n10\n9\n10\nFALSE\n\n\n8\n5121\n2980\n4/2/2017\n2/3/2009\nPrivate room\nNA\n1\n60\n50\n8\n9\n9\nFALSE\n\n\n9\n5172\n2980\n4/2/2017\n2/3/2009\nEntire home/apt\n1\n1\n129\n53\n9\n10\n9\nFALSE\n\n\n10\n5178\n2952\n4/2/2017\n3/3/2009\nPrivate room\n1\n1\n79\n329\n7\n10\n9\nFALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty Data\n\n\n\n\n\n\n\n\nFirst 10 rows of Blueprinty dataset\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\nMidwest\n32.5\n0\n\n\n3\nSouthwest\n37.5\n0\n\n\n4\nNorthwest\n27.0\n1\n\n\n3\nNortheast\n24.5\n0\n\n\n3\nSouthwest\n37.0\n0\n\n\n6\nNortheast\n29.5\n1\n\n\n5\nSouthwest\n27.0\n0\n\n\n5\nNortheast\n20.5\n0\n\n\n6\nNortheast\n25.0\n0\n\n\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nlibrary(ggplot2)\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(\n    values = c(\"0\" = \"#cce5df\", \"1\" = \"#a3c4dc\"),  # light teal and soft blue\n    labels = c(\"Non-Customer\", \"Customer\")\n  ) +\n  labs(\n    title = \"Distribution of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()+\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\nDistribution of Patents by Customer Status\n\n\n\n\n\nShow Code\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_patents = mean(patents, na.rm = TRUE),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Average Number of Patents by Customer Status\")\n\n\nAverage Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\ncount\n\n\n\n\n0\n3.473013\n1019\n\n\n1\n4.133056\n481\n\n\n\n\n\n\n\n\n\nObservation: Patents by Customer Status\n\n\n\nThe histogram and summary table suggest that Blueprinty customers tend to have more patents than non-customers:\n\nAverage patents: 4.13 for customers vs. 3.47 for non-customers\nThe histogram shows customers skewed toward higher patent counts\n\n\nHowever, further analysis is needed to determine if this difference is due to other factors like age or region."
  },
  {
    "objectID": "blog/project4/index.html#blueprinty-case-study",
    "href": "blog/project4/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Load required packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(knitr)\n\n# Read the datasets\nairbnb     &lt;- read_csv(\"/Users/sanjitkangovi/Desktop/mysite/blog/project4/airbnb.csv\")\nblueprinty &lt;- read_csv(\"/Users/sanjitkangovi/Desktop/mysite/blog/project4/blueprinty.csv\")\n\n\n\n\n\n\nAirbnb Data\n\n\n\n\n\n\n\n\nFirst 10 rows of Airbnb dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n…1\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1\n1\n59\n150\n9\n9\n9\nFALSE\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1\n0\n230\n20\n9\n10\n9\nFALSE\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1\n1\n150\n0\nNA\nNA\nNA\nFALSE\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1\n1\n89\n116\n9\n9\n9\nFALSE\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNA\n1\n39\n93\n9\n8\n9\nTRUE\n\n\n6\n5099\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1\n1\n212\n60\n9\n9\n9\nFALSE\n\n\n7\n5107\n2981\n4/2/2017\n2/2/2009\nEntire home/apt\n1\n2\n250\n60\n10\n9\n10\nFALSE\n\n\n8\n5121\n2980\n4/2/2017\n2/3/2009\nPrivate room\nNA\n1\n60\n50\n8\n9\n9\nFALSE\n\n\n9\n5172\n2980\n4/2/2017\n2/3/2009\nEntire home/apt\n1\n1\n129\n53\n9\n10\n9\nFALSE\n\n\n10\n5178\n2952\n4/2/2017\n3/3/2009\nPrivate room\n1\n1\n79\n329\n7\n10\n9\nFALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty Data\n\n\n\n\n\n\n\n\nFirst 10 rows of Blueprinty dataset\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\nMidwest\n32.5\n0\n\n\n3\nSouthwest\n37.5\n0\n\n\n4\nNorthwest\n27.0\n1\n\n\n3\nNortheast\n24.5\n0\n\n\n3\nSouthwest\n37.0\n0\n\n\n6\nNortheast\n29.5\n1\n\n\n5\nSouthwest\n27.0\n0\n\n\n5\nNortheast\n20.5\n0\n\n\n6\nNortheast\n25.0\n0\n\n\n4\nMidwest\n29.5\n0\n\n\n\n\n\n\n\n\n\n\n\nShow Code\nlibrary(ggplot2)\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\", color = \"black\") +\n  scale_fill_manual(\n    values = c(\"0\" = \"#cce5df\", \"1\" = \"#a3c4dc\"),  # light teal and soft blue\n    labels = c(\"Non-Customer\", \"Customer\")\n  ) +\n  labs(\n    title = \"Distribution of Patents by Customer Status\",\n    x = \"Number of Patents\",\n    y = \"Count\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal()+\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\nDistribution of Patents by Customer Status\n\n\n\n\n\nShow Code\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    mean_patents = mean(patents, na.rm = TRUE),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Average Number of Patents by Customer Status\")\n\n\nAverage Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\ncount\n\n\n\n\n0\n3.473013\n1019\n\n\n1\n4.133056\n481\n\n\n\n\n\n\n\n\n\nObservation: Patents by Customer Status\n\n\n\nThe histogram and summary table suggest that Blueprinty customers tend to have more patents than non-customers:\n\nAverage patents: 4.13 for customers vs. 3.47 for non-customers\nThe histogram shows customers skewed toward higher patent counts\n\n\nHowever, further analysis is needed to determine if this difference is due to other factors like age or region."
  },
  {
    "objectID": "blog/project4/index.html#comparison-of-patents-by-customer-status",
    "href": "blog/project4/index.html#comparison-of-patents-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Comparison of Patents by Customer Status",
    "text": "Comparison of Patents by Customer Status\n\n\n\nShow Code\nblueprinty %&gt;%\n  group_by(region, iscustomer) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  group_by(region) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  ggplot(aes(x = region, y = prop, fill = as.factor(iscustomer))) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"0\" = \"#cce5df\", \"1\" = \"#a3c4dc\"),\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(\n    title = \"Customer Distribution Across Regions\",\n    x = \"Region\",\n    y = \"Proportion\",\n    fill = \"Customer Status\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\n\n\n\n\nCustomer Distribution Across Regions\n\n\n\n\n\n\nShow Code\n#| echo: false\n#| message: false\n#| warning: false\n#| fig-cap: \"Age Distribution by Customer Status\"\n\nggplot(blueprinty, aes(x = as.factor(iscustomer), y = age, fill = as.factor(iscustomer))) +\n  geom_boxplot(alpha = 0.7) +\n  scale_fill_manual(values = c(\"0\" = \"#cce5df\", \"1\" = \"#a3c4dc\"),\n                    labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(\n    title = \"Age Distribution by Customer Status\",\n    x = \"Customer Status\",\n    y = \"Age\",\n    fill = \"Customer Status\"\n  ) +\n  scale_x_discrete(labels = c(\"0\" = \"Non-Customer\", \"1\" = \"Customer\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\n\n\n\n\nAge Distribution by Customer Status\n\n\n\n\n\nShow Code\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarise(\n    avg_age = mean(age, na.rm = TRUE),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Average Age by Customer Status\")\n\n\nAverage Age by Customer Status\n\n\niscustomer\navg_age\ncount\n\n\n\n\n0\n26.10157\n1019\n\n\n1\n26.90021\n481\n\n\n\n\n\n\n\n\n\nObservation: Region and Age Differences\n\n\n\nThere are clear systematic differences between Blueprinty customers and non-customers:\n\nRegion: Over two-thirds of customers are from the Northeast, while other regions like the Southwest and Midwest are underrepresented among customers.\nAge: Customers tend to be slightly older, with a higher median and average age than non-customers.\n\n\nThese differences suggest that both age and region could confound the observed relationship between customer status and number of patents. Controlling for these variables is important for causal inference.\n\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\). The probability mass function for each observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThen, the likelihood function for the entire sample is:\n\\[\n\\mathcal{L}(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nOr, more compactly:\n\\[\n\\mathcal{L}(\\lambda \\mid \\mathbf{Y}) = e^{-n\\lambda} \\lambda^{\\sum Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\nThe code for log-likelihood function for the Poisson model is\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  if (lambda &lt;= 0) return(-Inf)\n  sum(-lambda + Y * log(lambda) - lfactorial(Y))\n}\n\n\n\n\nShow Code\n# Use observed Y from your dataset\nY &lt;- blueprinty$patents\n\n# Range of lambda values to evaluate\nlambda_vals &lt;- seq(0.1, 10, by = 0.1)\n\n# Compute log-likelihood at each lambda\nloglik_vals &lt;- sapply(lambda_vals, function(lam) poisson_loglikelihood(lam, Y))\n\n# Plot\nplot(lambda_vals, loglik_vals, type = \"l\", lwd = 2,\n     xlab = expression(lambda), ylab = \"Log-Likelihood\",\n     main = \"Log-Likelihood Curve for Poisson Model\")\n\n\n\n\n\nLog-Likelihood of Poisson Model Across Values of λ\n\n\n\n\n\nLet \\(Y_1, Y_2, \\dots, Y_n \\overset{iid}{\\sim} \\text{Poisson}(\\lambda)\\), and recall that the log-likelihood function is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n= -n\\lambda + \\left(\\sum_{i=1}^n Y_i\\right) \\log \\lambda + \\text{const}\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\) and set it equal to zero:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\]\nSolving for \\(\\lambda\\) gives:\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense because the mean of a Poisson distribution is \\(\\lambda\\), so the sample mean \\(\\bar{Y}\\) is a natural estimator.\n\n\n# Define a negative log-likelihood (since optim minimizes)\nneg_loglik &lt;- function(lambda, Y) {\n  -poisson_loglikelihood(lambda, Y)\n}\n\n# Call optim to find the MLE for lambda\noptim_result &lt;- optim(\n  par = 1,                             # initial guess\n  fn = neg_loglik,\n  Y = blueprinty$patents,              # data passed to function\n  method = \"Brent\",\n  lower = 0.001, upper = 20            # bounds for lambda\n)\n\n# Print result\noptim_result$par  # this is the MLE for lambda\n\n[1] 3.684667\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # Compute the linear predictor: eta = X %*% beta\n  eta &lt;- X %*% beta\n\n  # Apply inverse link function: lambda_i = exp(eta_i)\n  lambda &lt;- exp(eta)\n\n  # Compute log-likelihood\n  loglik &lt;- sum(-lambda + Y * log(lambda) - lfactorial(Y))\n\n  return(loglik)\n}\n\n\n\n\n\n\n\n\nCode: MLE Estimation for Poisson Regression\n\n\n\n\n\n\n#| message: false\n#| warning: false\n\n# Step 1: Construct the covariate matrix X\n# Includes intercept, age, age², region dummies (omit 1), and iscustomer\nblueprinty$age_sq &lt;- blueprinty$age^2\nX &lt;- model.matrix(~ age + age_sq + region + iscustomer, data = blueprinty)\nY &lt;- blueprinty$patents\n\n# Step 2: Define negative log-likelihood for use with optim()\nneg_loglik_regression &lt;- function(beta, Y, X) {\n  eta &lt;- X %*% beta\n  lambda &lt;- exp(eta)\n  -sum(-lambda + Y * log(lambda) - lfactorial(Y))  # negative log-lik\n}\n\n# Step 3: Estimate β using optim\nbeta_init &lt;- rep(0, ncol(X))\noptim_result &lt;- optim(\n  par = beta_init,\n  fn = neg_loglik_regression,\n  Y = Y,\n  X = X,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n\n# Step 4: Extract β and standard errors\nbeta_hat &lt;- optim_result$par\nhessian &lt;- optim_result$hessian\nvar_cov_matrix &lt;- solve(hessian)\nse_beta &lt;- sqrt(diag(var_cov_matrix))\n\n# Step 5: Present results in a table\nresults &lt;- data.frame(\n  Coefficient = beta_hat,\n  Std_Error = se_beta,\n  row.names = colnames(X)\n)\n\n\n\n\nThe table below shows the maximum likelihood estimates and standard errors from the Poisson regression model.\n\n\n\nPoisson Regression Coefficients and Standard Errors\n\n\n\nCoefficient\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257359\n0.1122180\n\n\nage\n0.1157937\n0.0063574\n\n\nage_sq\n-0.0022287\n0.0000771\n\n\nregionNortheast\n-0.0245568\n0.0433763\n\n\nregionNorthwest\n-0.0348278\n0.0529311\n\n\nregionSouth\n-0.0054419\n0.0524007\n\n\nregionSouthwest\n-0.0377841\n0.0471722\n\n\niscustomer\n0.0606656\n0.0320588\n\n\n\n\n\n\n\nShow Code\n# Refit the model (if needed)\nglm_result &lt;- glm(\n  patents ~ age + I(age^2) + region + iscustomer,\n  family = poisson(link = \"log\"),\n  data = blueprinty\n)\n\n# Create tidy coefficient table\ncoef_table &lt;- data.frame(\n  Term = names(coef(glm_result)),\n  Estimate = round(coef(glm_result), 4),\n  Std_Error = round(sqrt(diag(vcov(glm_result))), 4),\n  z_value = round(coef(glm_result) / sqrt(diag(vcov(glm_result))), 2),\n  p_value = signif(2 * (1 - pnorm(abs(coef(glm_result) / sqrt(diag(vcov(glm_result)))))), 3)\n)\n\nFor comparison, the table below shows estimates and standard errors from R’s built-in glm() function using the Poisson family.\n\nGLM Poisson Regression Coefficients and Standard Errors\n\n\n\nCoefficient\nStd_Error\n\n\n\n\n(Intercept)\n-0.5089198\n0.1831787\n\n\nage\n0.1486195\n0.0138686\n\n\nI(age^2)\n-0.0029705\n0.0002580\n\n\nregionNortheast\n0.0291701\n0.0436255\n\n\nregionNorthwest\n-0.0175745\n0.0537806\n\n\nregionSouth\n0.0565613\n0.0526624\n\n\nregionSouthwest\n0.0505761\n0.0471982\n\n\niscustomer\n0.2075908\n0.0308953\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\n\n\n\n\nAge has a positive and significant effect on the number of patents, while age squared is negative — indicating diminishing returns to age on innovation.\nThe variable iscustomer is positive and statistically significant, suggesting that Blueprinty customers file more patents on average, even after controlling for age and region.\nMost regional variables are not significant, implying limited geographic variation once other covariates are accounted for.\n\nThese findings support the hypothesis that Blueprinty customers tend to be more patent-active, though part of this relationship may be explained by age.\n\n\n\n\nEstimate effect of Blueprinty’s software using counterfactual predictions\n# Step 1: Create X matrices for iscustomer = 0 and iscustomer = 1\nX_0 &lt;- model.matrix(~ age + I(age^2) + region + iscustomer,\n                    data = transform(blueprinty, iscustomer = 0))\nX_1 &lt;- model.matrix(~ age + I(age^2) + region + iscustomer,\n                    data = transform(blueprinty, iscustomer = 1))\n\n# Step 2: Predict lambda (expected number of patents) under both conditions\neta_0 &lt;- X_0 %*% coef(glm_result)\neta_1 &lt;- X_1 %*% coef(glm_result)\ny_pred_0 &lt;- exp(eta_0)\ny_pred_1 &lt;- exp(eta_1)\n\n# Step 3: Compute average difference\ndiff &lt;- y_pred_1 - y_pred_0\naverage_effect &lt;- mean(diff)\n\n# Display result\naverage_effect\n\n[1] 0.7927681\n\n\n\n\n\n\nInterpretation: Effect of Blueprinty’s Software\n\n\n\nThe average predicted increase in patents for a firm if it becomes a Blueprinty customer is 0.79 patents.\nThis quantifies the estimated causal effect of adopting the software, holding all other firm characteristics constant."
  },
  {
    "objectID": "blog/project4/index.html#airbnb-case-study",
    "href": "blog/project4/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\nData summary\n\n\nName\nairbnb\n\n\nNumber of rows\n40628\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlogical\n1\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlast_scraped\n0\n1\n8\n8\n0\n2\n0\n\n\nhost_since\n35\n1\n8\n10\n0\n2790\n0\n\n\nroom_type\n0\n1\n11\n15\n0\n3\n0\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\ninstant_bookable\n0\n1\n0.19\nFAL: 32759, TRU: 7869\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n…1\n0\n1.00\n20314.50\n11728.44\n1\n10157.75\n20314.5\n30471.25\n40628\n▇▇▇▇▇\n\n\nid\n0\n1.00\n9698889.00\n5460166.00\n2515\n4889868.25\n9862877.5\n14667893.75\n18009669\n▆▅▆▆▇\n\n\ndays\n0\n1.00\n1102.37\n1383.27\n1\n542.00\n996.0\n1535.00\n42828\n▇▁▁▁▁\n\n\nbathrooms\n160\n1.00\n1.12\n0.39\n0\n1.00\n1.0\n1.00\n8\n▇▁▁▁▁\n\n\nbedrooms\n76\n1.00\n1.15\n0.69\n0\n1.00\n1.0\n1.00\n10\n▇▁▁▁▁\n\n\nprice\n0\n1.00\n144.76\n210.66\n10\n70.00\n100.0\n170.00\n10000\n▇▁▁▁▁\n\n\nnumber_of_reviews\n0\n1.00\n15.90\n29.25\n0\n1.00\n4.0\n17.00\n421\n▇▁▁▁▁\n\n\nreview_scores_cleanliness\n10195\n0.75\n9.20\n1.12\n2\n9.00\n10.0\n10.00\n10\n▁▁▁▂▇\n\n\nreview_scores_location\n10254\n0.75\n9.41\n0.84\n2\n9.00\n10.0\n10.00\n10\n▁▁▁▁▇\n\n\nreview_scores_value\n10256\n0.75\n9.33\n0.90\n2\n9.00\n10.0\n10.00\n10\n▁▁▁▁▇\n\n\n\n\n\n\n\n\n\n\nLine plot: Average Reviews by Price Bin\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create price bins\nairbnb_binned &lt;- airbnb %&gt;%\n  filter(price &gt; 0, price &lt;= 1000) %&gt;%  # remove extreme values\n  mutate(price_bin = cut(price, breaks = seq(0, 1000, by = 50), include.lowest = TRUE)) %&gt;%\n  group_by(price_bin) %&gt;%\n  summarise(avg_reviews = mean(number_of_reviews, na.rm = TRUE)) %&gt;%\n  na.omit()\n\n# Plot\nggplot(airbnb_binned, aes(x = price_bin, y = avg_reviews, group = 1)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_point(color = \"steelblue\") +\n  labs(\n    title = \"Average Number of Reviews by Price\",\n    x = \"Price Range (USD)\",\n    y = \"Average Reviews\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nDensity plot: Zoomed-in Number of Reviews\nggplot(airbnb, aes(x = number_of_reviews)) +\n  geom_density(fill = \"#a3c4dc\", alpha = 0.7) +\n  coord_cartesian(xlim = c(0, 100)) +  # zoom in without removing data\n  labs(\n    title = \"Density of Number of Reviews (Zoomed In)\",\n    x = \"Number of Reviews\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nHandle missing values\n# Remove rows with missing values in relevant predictors\nairbnb_clean &lt;- airbnb %&gt;%\n  filter(\n    !is.na(bathrooms),\n    !is.na(bedrooms),\n    !is.na(price),\n    !is.na(review_scores_cleanliness),\n    !is.na(review_scores_location),\n    !is.na(review_scores_value),\n    !is.na(number_of_reviews),\n    !is.na(instant_bookable),\n    !is.na(room_type)\n  )\n\n\n\nFit Poisson regression model for number of reviews\n# Convert categorical to factors\nairbnb_clean &lt;- airbnb_clean %&gt;%\n  mutate(\n    instant_bookable = factor(instant_bookable),\n    room_type = factor(room_type)\n  )\n\n# Fit Poisson model\nreview_model &lt;- glm(\n  number_of_reviews ~ price + bathrooms + bedrooms +\n    review_scores_cleanliness + review_scores_location + review_scores_value +\n    instant_bookable + room_type,\n  family = poisson(link = \"log\"),\n  data = airbnb_clean\n)\n\n# Summarize as clean table\ncoef_table &lt;- data.frame(\n  Term = names(coef(review_model)),\n  Estimate = round(coef(review_model), 4),\n  Std_Error = round(sqrt(diag(vcov(review_model))), 4),\n  z_value = round(coef(review_model) / sqrt(diag(vcov(review_model))), 2),\n  p_value = signif(2 * (1 - pnorm(abs(coef(review_model) / sqrt(diag(vcov(review_model)))))), 3)\n)\n\nknitr::kable(coef_table, caption = \"Poisson Regression: Predicting Number of Reviews\")\n\n\nPoisson Regression: Predicting Number of Reviews\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd_Error\nz_value\np_value\n\n\n\n\n(Intercept)\n(Intercept)\n3.5725\n0.0160\n223.21\n0.00e+00\n\n\nprice\nprice\n0.0000\n0.0000\n-1.73\n8.38e-02\n\n\nbathrooms\nbathrooms\n-0.1240\n0.0037\n-33.09\n0.00e+00\n\n\nbedrooms\nbedrooms\n0.0749\n0.0020\n37.70\n0.00e+00\n\n\nreview_scores_cleanliness\nreview_scores_cleanliness\n0.1132\n0.0015\n75.82\n0.00e+00\n\n\nreview_scores_location\nreview_scores_location\n-0.0768\n0.0016\n-47.80\n0.00e+00\n\n\nreview_scores_value\nreview_scores_value\n-0.0915\n0.0018\n-50.90\n0.00e+00\n\n\ninstant_bookableTRUE\ninstant_bookableTRUE\n0.3344\n0.0029\n115.75\n0.00e+00\n\n\nroom_typePrivate room\nroom_typePrivate room\n-0.0145\n0.0027\n-5.31\n1.00e-07\n\n\nroom_typeShared room\nroom_typeShared room\n-0.2519\n0.0086\n-29.23\n0.00e+00\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Model\n\n\n\n\nUnits with higher cleanliness, location, and value scores tend to have significantly more reviews, suggesting they receive more bookings.\nRoom type and instant bookable also matter — for example, entire homes may receive more reviews than shared rooms.\nThe coefficient on price reflects the expected log change in review count for a one-dollar increase in price, but note this may be nonlinear and confounded.\n\nThese results suggest clear variation in review count as a function of Airbnb listing features."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project3/index.html#introduction",
    "href": "blog/project3/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project3/index.html#data",
    "href": "blog/project3/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\nlibrary(haven)\nlibrary(dplyr)\ndata &lt;- read_dta(\"/Users/sanjitkangovi/Desktop/mysite/blog/project3/karlan_list_2007.dta\")\n\nDescription\n\n\n\n\n\n\n\nStructure of the Dataset\n\n\n\n\n\n\nglimpse(data)\n\nRows: 50,083\nColumns: 51\n$ treatment          &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, …\n$ control            &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, …\n$ ratio              &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 1,…\n$ ratio2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, …\n$ ratio3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ size               &lt;dbl+lbl&gt; 0, 0, 3, 4, 2, 0, 1, 3, 4, 1, 4, 2, 0, 1, 0, 4,…\n$ size25             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ size50             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ size100            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ sizeno             &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ask                &lt;dbl+lbl&gt; 0, 0, 1, 1, 1, 0, 3, 3, 2, 2, 1, 3, 0, 2, 0, 1,…\n$ askd1              &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ askd2              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, …\n$ askd3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ ask1               &lt;dbl&gt; 55, 25, 55, 55, 35, 95, 125, 75, 250, 150, 125, 25,…\n$ ask2               &lt;dbl&gt; 70, 35, 70, 70, 45, 120, 160, 95, 315, 190, 160, 35…\n$ ask3               &lt;dbl&gt; 85, 50, 85, 85, 55, 145, 190, 120, 375, 225, 190, 5…\n$ amount             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gave               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ amountchange       &lt;dbl&gt; -45, -25, -50, -25, -15, -45, -50, -65, -100, -125,…\n$ hpa                &lt;dbl&gt; 45, 25, 50, 50, 25, 90, 100, 65, 200, 125, 100, 5, …\n$ ltmedmra           &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ freq               &lt;dbl&gt; 2, 2, 3, 15, 42, 20, 12, 13, 28, 4, 1, 1, 2, 80, 3,…\n$ years              &lt;dbl&gt; 4, 3, 2, 8, 95, 10, 8, 16, 19, 7, 3, 1, 6, 19, 3, 1…\n$ year5              &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, …\n$ mrm2               &lt;dbl&gt; 31, 5, 6, 1, 24, 3, 4, 4, 6, 35, 41, 8, 28, 15, 5, …\n$ dormant            &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ female             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ couple             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ state50one         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nonlit             &lt;dbl&gt; 5, 0, 3, 1, 1, 0, 0, 4, 1, 4, 4, 1, 1, 4, 0, 3, 6, …\n$ cases              &lt;dbl&gt; 4, 2, 1, 2, 1, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 2, …\n$ statecnt           &lt;dbl&gt; 4.5002995, 2.9822462, 9.6070213, 3.2814682, 2.30201…\n$ stateresponse      &lt;dbl&gt; 0.01994681, 0.02608696, 0.02304817, 0.02066869, 0.0…\n$ stateresponset     &lt;dbl&gt; 0.019502353, 0.027833002, 0.022158911, 0.024702653,…\n$ stateresponsec     &lt;dbl&gt; 0.020806242, 0.022494888, 0.024743512, 0.012681159,…\n$ stateresponsetminc &lt;dbl&gt; -0.001303889, 0.005338114, -0.002584601, 0.01202149…\n$ perbush            &lt;dbl&gt; 0.4900000, 0.4646465, 0.4081633, 0.4646465, 0.52525…\n$ close25            &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ red0               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, …\n$ blue0              &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, …\n$ redcty             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, …\n$ bluecty            &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ pwhite             &lt;dbl&gt; 0.4464934, NA, 0.9357064, 0.8883309, 0.7590141, 0.8…\n$ pblack             &lt;dbl&gt; 0.527769208, NA, 0.011948366, 0.010760401, 0.127420…\n$ page18_39          &lt;dbl&gt; 0.3175913, NA, 0.2761282, 0.2794118, 0.4423889, 0.3…\n$ ave_hh_sz          &lt;dbl&gt; 2.10, NA, 2.48, 2.65, 1.85, 2.92, 2.10, 2.47, 2.49,…\n$ median_hhincome    &lt;dbl&gt; 28517, NA, 51175, 79269, 40908, 61779, 54655, 14152…\n$ powner             &lt;dbl&gt; 0.4998072, NA, 0.7219406, 0.9204314, 0.4160721, 0.9…\n$ psch_atlstba       &lt;dbl&gt; 0.32452780, NA, 0.19266793, 0.41214216, 0.43996516,…\n$ pop_propurban      &lt;dbl&gt; 1.0000000, NA, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n\n\n\n\n\n\n\n\n\n\nSummary of Numeric Variables\n\n\n\n\n\n\nsummary_df &lt;- as.data.frame(summary(select(data, where(is.numeric))))\nkable(summary_df, caption = \"Summary Statistics\")\n\n\nSummary Statistics\n\n\nVar1\nVar2\nFreq\n\n\n\n\n\ntreatment\nMin. :0.0000\n\n\n\ntreatment\n1st Qu.:0.0000\n\n\n\ntreatment\nMedian :1.0000\n\n\n\ntreatment\nMean :0.6668\n\n\n\ntreatment\n3rd Qu.:1.0000\n\n\n\ntreatment\nMax. :1.0000\n\n\n\ntreatment\nNA\n\n\n\ncontrol\nMin. :0.0000\n\n\n\ncontrol\n1st Qu.:0.0000\n\n\n\ncontrol\nMedian :0.0000\n\n\n\ncontrol\nMean :0.3332\n\n\n\ncontrol\n3rd Qu.:1.0000\n\n\n\ncontrol\nMax. :1.0000\n\n\n\ncontrol\nNA\n\n\n\nratio\nMin. :0.000\n\n\n\nratio\n1st Qu.:0.000\n\n\n\nratio\nMedian :1.000\n\n\n\nratio\nMean :1.334\n\n\n\nratio\n3rd Qu.:2.000\n\n\n\nratio\nMax. :3.000\n\n\n\nratio\nNA\n\n\n\nratio2\nMin. :0.0000\n\n\n\nratio2\n1st Qu.:0.0000\n\n\n\nratio2\nMedian :0.0000\n\n\n\nratio2\nMean :0.2223\n\n\n\nratio2\n3rd Qu.:0.0000\n\n\n\nratio2\nMax. :1.0000\n\n\n\nratio2\nNA\n\n\n\nratio3\nMin. :0.0000\n\n\n\nratio3\n1st Qu.:0.0000\n\n\n\nratio3\nMedian :0.0000\n\n\n\nratio3\nMean :0.2222\n\n\n\nratio3\n3rd Qu.:0.0000\n\n\n\nratio3\nMax. :1.0000\n\n\n\nratio3\nNA\n\n\n\nsize\nMin. :0.000\n\n\n\nsize\n1st Qu.:0.000\n\n\n\nsize\nMedian :2.000\n\n\n\nsize\nMean :1.667\n\n\n\nsize\n3rd Qu.:3.000\n\n\n\nsize\nMax. :4.000\n\n\n\nsize\nNA\n\n\n\nsize25\nMin. :0.0000\n\n\n\nsize25\n1st Qu.:0.0000\n\n\n\nsize25\nMedian :0.0000\n\n\n\nsize25\nMean :0.1667\n\n\n\nsize25\n3rd Qu.:0.0000\n\n\n\nsize25\nMax. :1.0000\n\n\n\nsize25\nNA\n\n\n\nsize50\nMin. :0.0000\n\n\n\nsize50\n1st Qu.:0.0000\n\n\n\nsize50\nMedian :0.0000\n\n\n\nsize50\nMean :0.1666\n\n\n\nsize50\n3rd Qu.:0.0000\n\n\n\nsize50\nMax. :1.0000\n\n\n\nsize50\nNA\n\n\n\nsize100\nMin. :0.0000\n\n\n\nsize100\n1st Qu.:0.0000\n\n\n\nsize100\nMedian :0.0000\n\n\n\nsize100\nMean :0.1667\n\n\n\nsize100\n3rd Qu.:0.0000\n\n\n\nsize100\nMax. :1.0000\n\n\n\nsize100\nNA\n\n\n\nsizeno\nMin. :0.0000\n\n\n\nsizeno\n1st Qu.:0.0000\n\n\n\nsizeno\nMedian :0.0000\n\n\n\nsizeno\nMean :0.1667\n\n\n\nsizeno\n3rd Qu.:0.0000\n\n\n\nsizeno\nMax. :1.0000\n\n\n\nsizeno\nNA\n\n\n\nask\nMin. :0.000\n\n\n\nask\n1st Qu.:0.000\n\n\n\nask\nMedian :1.000\n\n\n\nask\nMean :1.334\n\n\n\nask\n3rd Qu.:2.000\n\n\n\nask\nMax. :3.000\n\n\n\nask\nNA\n\n\n\naskd1\nMin. :0.0000\n\n\n\naskd1\n1st Qu.:0.0000\n\n\n\naskd1\nMedian :0.0000\n\n\n\naskd1\nMean :0.2223\n\n\n\naskd1\n3rd Qu.:0.0000\n\n\n\naskd1\nMax. :1.0000\n\n\n\naskd1\nNA\n\n\n\naskd2\nMin. :0.0000\n\n\n\naskd2\n1st Qu.:0.0000\n\n\n\naskd2\nMedian :0.0000\n\n\n\naskd2\nMean :0.2223\n\n\n\naskd2\n3rd Qu.:0.0000\n\n\n\naskd2\nMax. :1.0000\n\n\n\naskd2\nNA\n\n\n\naskd3\nMin. :0.0000\n\n\n\naskd3\n1st Qu.:0.0000\n\n\n\naskd3\nMedian :0.0000\n\n\n\naskd3\nMean :0.2222\n\n\n\naskd3\n3rd Qu.:0.0000\n\n\n\naskd3\nMax. :1.0000\n\n\n\naskd3\nNA\n\n\n\nask1\nMin. : 25.0\n\n\n\nask1\n1st Qu.: 35.0\n\n\n\nask1\nMedian : 45.0\n\n\n\nask1\nMean : 71.5\n\n\n\nask1\n3rd Qu.: 65.0\n\n\n\nask1\nMax. :1500.0\n\n\n\nask1\nNA\n\n\n\nask2\nMin. : 35.00\n\n\n\nask2\n1st Qu.: 45.00\n\n\n\nask2\nMedian : 60.00\n\n\n\nask2\nMean : 91.79\n\n\n\nask2\n3rd Qu.: 85.00\n\n\n\nask2\nMax. :1875.00\n\n\n\nask2\nNA\n\n\n\nask3\nMin. : 50\n\n\n\nask3\n1st Qu.: 55\n\n\n\nask3\nMedian : 70\n\n\n\nask3\nMean : 111\n\n\n\nask3\n3rd Qu.: 100\n\n\n\nask3\nMax. :2250\n\n\n\nask3\nNA\n\n\n\namount\nMin. : 0.0000\n\n\n\namount\n1st Qu.: 0.0000\n\n\n\namount\nMedian : 0.0000\n\n\n\namount\nMean : 0.9157\n\n\n\namount\n3rd Qu.: 0.0000\n\n\n\namount\nMax. :400.0000\n\n\n\namount\nNA\n\n\n\ngave\nMin. :0.00000\n\n\n\ngave\n1st Qu.:0.00000\n\n\n\ngave\nMedian :0.00000\n\n\n\ngave\nMean :0.02065\n\n\n\ngave\n3rd Qu.:0.00000\n\n\n\ngave\nMax. :1.00000\n\n\n\ngave\nNA\n\n\n\namountchange\nMin. :-200412.12\n\n\n\namountchange\n1st Qu.: -50.00\n\n\n\namountchange\nMedian : -30.00\n\n\n\namountchange\nMean : -52.67\n\n\n\namountchange\n3rd Qu.: -25.00\n\n\n\namountchange\nMax. : 275.00\n\n\n\namountchange\nNA\n\n\n\nhpa\nMin. : 0.00\n\n\n\nhpa\n1st Qu.: 30.00\n\n\n\nhpa\nMedian : 45.00\n\n\n\nhpa\nMean : 59.38\n\n\n\nhpa\n3rd Qu.: 60.00\n\n\n\nhpa\nMax. :1000.00\n\n\n\nhpa\nNA\n\n\n\nltmedmra\nMin. :0.0000\n\n\n\nltmedmra\n1st Qu.:0.0000\n\n\n\nltmedmra\nMedian :0.0000\n\n\n\nltmedmra\nMean :0.4937\n\n\n\nltmedmra\n3rd Qu.:1.0000\n\n\n\nltmedmra\nMax. :1.0000\n\n\n\nltmedmra\nNA\n\n\n\nfreq\nMin. : 0.000\n\n\n\nfreq\n1st Qu.: 2.000\n\n\n\nfreq\nMedian : 4.000\n\n\n\nfreq\nMean : 8.039\n\n\n\nfreq\n3rd Qu.: 10.000\n\n\n\nfreq\nMax. :218.000\n\n\n\nfreq\nNA\n\n\n\nyears\nMin. : 0.000\n\n\n\nyears\n1st Qu.: 2.000\n\n\n\nyears\nMedian : 5.000\n\n\n\nyears\nMean : 6.098\n\n\n\nyears\n3rd Qu.: 9.000\n\n\n\nyears\nMax. :95.000\n\n\n\nyears\nNA’s :1\n\n\n\nyear5\nMin. :0.0000\n\n\n\nyear5\n1st Qu.:0.0000\n\n\n\nyear5\nMedian :1.0000\n\n\n\nyear5\nMean :0.5088\n\n\n\nyear5\n3rd Qu.:1.0000\n\n\n\nyear5\nMax. :1.0000\n\n\n\nyear5\nNA\n\n\n\nmrm2\nMin. : 0.00\n\n\n\nmrm2\n1st Qu.: 4.00\n\n\n\nmrm2\nMedian : 8.00\n\n\n\nmrm2\nMean : 13.01\n\n\n\nmrm2\n3rd Qu.: 19.00\n\n\n\nmrm2\nMax. :168.00\n\n\n\nmrm2\nNA’s :1\n\n\n\ndormant\nMin. :0.0000\n\n\n\ndormant\n1st Qu.:0.0000\n\n\n\ndormant\nMedian :1.0000\n\n\n\ndormant\nMean :0.5235\n\n\n\ndormant\n3rd Qu.:1.0000\n\n\n\ndormant\nMax. :1.0000\n\n\n\ndormant\nNA\n\n\n\nfemale\nMin. :0.0000\n\n\n\nfemale\n1st Qu.:0.0000\n\n\n\nfemale\nMedian :0.0000\n\n\n\nfemale\nMean :0.2777\n\n\n\nfemale\n3rd Qu.:1.0000\n\n\n\nfemale\nMax. :1.0000\n\n\n\nfemale\nNA’s :1111\n\n\n\ncouple\nMin. :0.0000\n\n\n\ncouple\n1st Qu.:0.0000\n\n\n\ncouple\nMedian :0.0000\n\n\n\ncouple\nMean :0.0919\n\n\n\ncouple\n3rd Qu.:0.0000\n\n\n\ncouple\nMax. :1.0000\n\n\n\ncouple\nNA’s :1148\n\n\n\nstate50one\nMin. :0.0000000\n\n\n\nstate50one\n1st Qu.:0.0000000\n\n\n\nstate50one\nMedian :0.0000000\n\n\n\nstate50one\nMean :0.0009983\n\n\n\nstate50one\n3rd Qu.:0.0000000\n\n\n\nstate50one\nMax. :1.0000000\n\n\n\nstate50one\nNA\n\n\n\nnonlit\nMin. :0.000\n\n\n\nnonlit\n1st Qu.:1.000\n\n\n\nnonlit\nMedian :3.000\n\n\n\nnonlit\nMean :2.474\n\n\n\nnonlit\n3rd Qu.:4.000\n\n\n\nnonlit\nMax. :6.000\n\n\n\nnonlit\nNA’s :452\n\n\n\ncases\nMin. :0.0\n\n\n\ncases\n1st Qu.:1.0\n\n\n\ncases\nMedian :1.0\n\n\n\ncases\nMean :1.5\n\n\n\ncases\n3rd Qu.:2.0\n\n\n\ncases\nMax. :4.0\n\n\n\ncases\nNA’s :452\n\n\n\nstatecnt\nMin. : 0.001995\n\n\n\nstatecnt\n1st Qu.: 1.833234\n\n\n\nstatecnt\nMedian : 3.538799\n\n\n\nstatecnt\nMean : 5.998820\n\n\n\nstatecnt\n3rd Qu.: 9.607021\n\n\n\nstatecnt\nMax. :17.368841\n\n\n\nstatecnt\nNA\n\n\n\nstateresponse\nMin. :0.00000\n\n\n\nstateresponse\n1st Qu.:0.01816\n\n\n\nstateresponse\nMedian :0.01971\n\n\n\nstateresponse\nMean :0.02063\n\n\n\nstateresponse\n3rd Qu.:0.02305\n\n\n\nstateresponse\nMax. :0.07692\n\n\n\nstateresponse\nNA\n\n\n\nstateresponset\nMin. :0.00000\n\n\n\nstateresponset\n1st Qu.:0.01849\n\n\n\nstateresponset\nMedian :0.02170\n\n\n\nstateresponset\nMean :0.02199\n\n\n\nstateresponset\n3rd Qu.:0.02470\n\n\n\nstateresponset\nMax. :0.11111\n\n\n\nstateresponset\nNA\n\n\n\nstateresponsec\nMin. :0.00000\n\n\n\nstateresponsec\n1st Qu.:0.01286\n\n\n\nstateresponsec\nMedian :0.01988\n\n\n\nstateresponsec\nMean :0.01772\n\n\n\nstateresponsec\n3rd Qu.:0.02081\n\n\n\nstateresponsec\nMax. :0.05263\n\n\n\nstateresponsec\nNA’s :3\n\n\n\nstateresponsetminc\nMin. :-0.047619\n\n\n\nstateresponsetminc\n1st Qu.:-0.001388\n\n\n\nstateresponsetminc\nMedian : 0.001779\n\n\n\nstateresponsetminc\nMean : 0.004273\n\n\n\nstateresponsetminc\n3rd Qu.: 0.010545\n\n\n\nstateresponsetminc\nMax. : 0.111111\n\n\n\nstateresponsetminc\nNA’s :3\n\n\n\nperbush\nMin. :0.09091\n\n\n\nperbush\n1st Qu.:0.44444\n\n\n\nperbush\nMedian :0.48485\n\n\n\nperbush\nMean :0.48794\n\n\n\nperbush\n3rd Qu.:0.52525\n\n\n\nperbush\nMax. :0.73196\n\n\n\nperbush\nNA’s :35\n\n\n\nclose25\nMin. :0.0000\n\n\n\nclose25\n1st Qu.:0.0000\n\n\n\nclose25\nMedian :0.0000\n\n\n\nclose25\nMean :0.1857\n\n\n\nclose25\n3rd Qu.:0.0000\n\n\n\nclose25\nMax. :1.0000\n\n\n\nclose25\nNA’s :35\n\n\n\nred0\nMin. :0.0000\n\n\n\nred0\n1st Qu.:0.0000\n\n\n\nred0\nMedian :0.0000\n\n\n\nred0\nMean :0.4045\n\n\n\nred0\n3rd Qu.:1.0000\n\n\n\nred0\nMax. :1.0000\n\n\n\nred0\nNA’s :35\n\n\n\nblue0\nMin. :0.0000\n\n\n\nblue0\n1st Qu.:0.0000\n\n\n\nblue0\nMedian :1.0000\n\n\n\nblue0\nMean :0.5955\n\n\n\nblue0\n3rd Qu.:1.0000\n\n\n\nblue0\nMax. :1.0000\n\n\n\nblue0\nNA’s :35\n\n\n\nredcty\nMin. :0.0000\n\n\n\nredcty\n1st Qu.:0.0000\n\n\n\nredcty\nMedian :1.0000\n\n\n\nredcty\nMean :0.5102\n\n\n\nredcty\n3rd Qu.:1.0000\n\n\n\nredcty\nMax. :1.0000\n\n\n\nredcty\nNA’s :105\n\n\n\nbluecty\nMin. :0.0000\n\n\n\nbluecty\n1st Qu.:0.0000\n\n\n\nbluecty\nMedian :0.0000\n\n\n\nbluecty\nMean :0.4887\n\n\n\nbluecty\n3rd Qu.:1.0000\n\n\n\nbluecty\nMax. :1.0000\n\n\n\nbluecty\nNA’s :105\n\n\n\npwhite\nMin. :0.00942\n\n\n\npwhite\n1st Qu.:0.75584\n\n\n\npwhite\nMedian :0.87280\n\n\n\npwhite\nMean :0.81960\n\n\n\npwhite\n3rd Qu.:0.93883\n\n\n\npwhite\nMax. :1.00000\n\n\n\npwhite\nNA’s :1866\n\n\n\npblack\nMin. :0.00000\n\n\n\npblack\n1st Qu.:0.01473\n\n\n\npblack\nMedian :0.03655\n\n\n\npblack\nMean :0.08671\n\n\n\npblack\n3rd Qu.:0.09088\n\n\n\npblack\nMax. :0.98962\n\n\n\npblack\nNA’s :2036\n\n\n\npage18_39\nMin. :0.0000\n\n\n\npage18_39\n1st Qu.:0.2583\n\n\n\npage18_39\nMedian :0.3055\n\n\n\npage18_39\nMean :0.3217\n\n\n\npage18_39\n3rd Qu.:0.3691\n\n\n\npage18_39\nMax. :0.9975\n\n\n\npage18_39\nNA’s :1866\n\n\n\nave_hh_sz\nMin. :0.000\n\n\n\nave_hh_sz\n1st Qu.:2.210\n\n\n\nave_hh_sz\nMedian :2.440\n\n\n\nave_hh_sz\nMean :2.429\n\n\n\nave_hh_sz\n3rd Qu.:2.660\n\n\n\nave_hh_sz\nMax. :5.270\n\n\n\nave_hh_sz\nNA’s :1862\n\n\n\nmedian_hhincome\nMin. : 5000\n\n\n\nmedian_hhincome\n1st Qu.: 39181\n\n\n\nmedian_hhincome\nMedian : 50673\n\n\n\nmedian_hhincome\nMean : 54816\n\n\n\nmedian_hhincome\n3rd Qu.: 66005\n\n\n\nmedian_hhincome\nMax. :200001\n\n\n\nmedian_hhincome\nNA’s :1874\n\n\n\npowner\nMin. :0.0000\n\n\n\npowner\n1st Qu.:0.5602\n\n\n\npowner\nMedian :0.7123\n\n\n\npowner\nMean :0.6694\n\n\n\npowner\n3rd Qu.:0.8168\n\n\n\npowner\nMax. :1.0000\n\n\n\npowner\nNA’s :1869\n\n\n\npsch_atlstba\nMin. :0.0000\n\n\n\npsch_atlstba\n1st Qu.:0.2356\n\n\n\npsch_atlstba\nMedian :0.3737\n\n\n\npsch_atlstba\nMean :0.3917\n\n\n\npsch_atlstba\n3rd Qu.:0.5300\n\n\n\npsch_atlstba\nMax. :1.0000\n\n\n\npsch_atlstba\nNA’s :1868\n\n\n\npop_propurban\nMin. :0.0000\n\n\n\npop_propurban\n1st Qu.:0.8849\n\n\n\npop_propurban\nMedian :1.0000\n\n\n\npop_propurban\nMean :0.8720\n\n\n\npop_propurban\n3rd Qu.:1.0000\n\n\n\npop_propurban\nMax. :1.0000\n\n\n\npop_propurban\nNA’s :1866\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\n\n\nmissing_df &lt;- data.frame(\n  Variable = names(data),\n  Missing = colSums(is.na(data))\n)\nkable(missing_df, caption = \"Missing Values Per Variable\")\n\n\nMissing Values Per Variable\n\n\n\nVariable\nMissing\n\n\n\n\ntreatment\ntreatment\n0\n\n\ncontrol\ncontrol\n0\n\n\nratio\nratio\n0\n\n\nratio2\nratio2\n0\n\n\nratio3\nratio3\n0\n\n\nsize\nsize\n0\n\n\nsize25\nsize25\n0\n\n\nsize50\nsize50\n0\n\n\nsize100\nsize100\n0\n\n\nsizeno\nsizeno\n0\n\n\nask\nask\n0\n\n\naskd1\naskd1\n0\n\n\naskd2\naskd2\n0\n\n\naskd3\naskd3\n0\n\n\nask1\nask1\n0\n\n\nask2\nask2\n0\n\n\nask3\nask3\n0\n\n\namount\namount\n0\n\n\ngave\ngave\n0\n\n\namountchange\namountchange\n0\n\n\nhpa\nhpa\n0\n\n\nltmedmra\nltmedmra\n0\n\n\nfreq\nfreq\n0\n\n\nyears\nyears\n1\n\n\nyear5\nyear5\n0\n\n\nmrm2\nmrm2\n1\n\n\ndormant\ndormant\n0\n\n\nfemale\nfemale\n1111\n\n\ncouple\ncouple\n1148\n\n\nstate50one\nstate50one\n0\n\n\nnonlit\nnonlit\n452\n\n\ncases\ncases\n452\n\n\nstatecnt\nstatecnt\n0\n\n\nstateresponse\nstateresponse\n0\n\n\nstateresponset\nstateresponset\n0\n\n\nstateresponsec\nstateresponsec\n3\n\n\nstateresponsetminc\nstateresponsetminc\n3\n\n\nperbush\nperbush\n35\n\n\nclose25\nclose25\n35\n\n\nred0\nred0\n35\n\n\nblue0\nblue0\n35\n\n\nredcty\nredcty\n105\n\n\nbluecty\nbluecty\n105\n\n\npwhite\npwhite\n1866\n\n\npblack\npblack\n2036\n\n\npage18_39\npage18_39\n1866\n\n\nave_hh_sz\nave_hh_sz\n1862\n\n\nmedian_hhincome\nmedian_hhincome\n1874\n\n\npowner\npowner\n1869\n\n\npsch_atlstba\npsch_atlstba\n1868\n\n\npop_propurban\npop_propurban\n1866\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary by Treatment Group\n\n\n\n\n\n\ngrouped_summary &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    response_rate = mean(gave, na.rm = TRUE),\n    avg_donation = mean(amount, na.rm = TRUE),\n    n = n()\n  )\nkable(grouped_summary, caption = \"Summary by Treatment Group\")\n\n\nSummary by Treatment Group\n\n\ntreatment\nresponse_rate\navg_donation\nn\n\n\n\n\n0\n0.0178582\n0.8132678\n16687\n\n\n1\n0.0220386\n0.9668733\n33396\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\n\n\n\n\n\nBalance Test: mrm2 (Months Since Last Donation)\n\n\n\n\n\n\n# Subset to remove missing values\ndf &lt;- data %&gt;% filter(!is.na(mrm2), !is.na(treatment))\n\n# T-test using base R (Welch’s t-test, unequal variances)\nt_test_result &lt;- t.test(mrm2 ~ treatment, data = df)\nprint(t_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\n# Linear regression: mrm2 ~ treatment\nreg_result &lt;- lm(mrm2 ~ treatment, data = df)\nsummary(reg_result)\n\n\nCall:\nlm(formula = mrm2 ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.012  -9.012  -5.012   6.002 154.988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.99814    0.09353 138.979   &lt;2e-16 ***\ntreatment    0.01369    0.11453   0.119    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.08 on 50080 degrees of freedom\nMultiple R-squared:  2.851e-07, Adjusted R-squared:  -1.968e-05 \nF-statistic: 0.01428 on 1 and 50080 DF,  p-value: 0.9049\n\n# Mean difference directly\nmean_diff &lt;- with(df, mean(mrm2[treatment == 1]) - mean(mrm2[treatment == 0]))\ncat(\"Mean difference (Treatment - Control):\", round(mean_diff, 3), \"\\n\")\n\nMean difference (Treatment - Control): 0.014 \n\n\n\n\n\n\n\n\n\n\n\nBalance Test: freq (Donation Frequency)\n\n\n\n\n\n\n# Subset data to remove missing values for freq and treatment\ndf &lt;- data %&gt;% filter(!is.na(freq), !is.na(treatment))\n\n# T-test (Welch's two-sample t-test for unequal variances)\nt_test_freq &lt;- t.test(freq ~ treatment, data = df)\nprint(t_test_freq)\n\n\n    Welch Two Sample t-test\n\ndata:  freq by treatment\nt = 0.11085, df = 33326, p-value = 0.9117\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.1998370  0.2237945\nsample estimates:\nmean in group 0 mean in group 1 \n       8.047342        8.035364 \n\n# Linear regression: freq ~ treatment\nlm_freq &lt;- lm(freq ~ treatment, data = df)\nsummary(lm_freq)\n\n\nCall:\nlm(formula = freq ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -8.035  -6.047  -4.035   1.953 209.965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.04734    0.08821  91.231   &lt;2e-16 ***\ntreatment   -0.01198    0.10802  -0.111    0.912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.39 on 50081 degrees of freedom\nMultiple R-squared:  2.455e-07, Adjusted R-squared:  -1.972e-05 \nF-statistic: 0.0123 on 1 and 50081 DF,  p-value: 0.9117\n\n# Optional: manually calculate mean difference\nmean_diff &lt;- with(df, mean(freq[treatment == 1]) - mean(freq[treatment == 0]))\ncat(\"Mean difference (Treatment - Control):\", round(mean_diff, 3), \"\\n\")\n\nMean difference (Treatment - Control): -0.012 \n\n\n\n\n\n\n\n\n\n\n\nBalance Test: dormant (Inactivity Indicator)\n\n\n\n\n\n\n# Filter out missing values\ndf &lt;- data %&gt;% filter(!is.na(dormant), !is.na(treatment))\n\n# T-test (Welch's test for difference in proportions since dormant is binary)\nt_test_dormant &lt;- t.test(dormant ~ treatment, data = df)\nprint(t_test_dormant)\n\n\n    Welch Two Sample t-test\n\ndata:  dormant by treatment\nt = -0.17388, df = 33362, p-value = 0.862\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.010104126  0.008457478\nsample estimates:\nmean in group 0 mean in group 1 \n      0.5229220       0.5237454 \n\n# Linear regression: dormant ~ treatment\nlm_dormant &lt;- lm(dormant ~ treatment, data = df)\nsummary(lm_dormant)\n\n\nCall:\nlm(formula = dormant ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5238 -0.5238  0.4763  0.4763  0.4771 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.5229220  0.0038664 135.247   &lt;2e-16 ***\ntreatment   0.0008233  0.0047349   0.174    0.862    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4995 on 50081 degrees of freedom\nMultiple R-squared:  6.037e-07, Adjusted R-squared:  -1.936e-05 \nF-statistic: 0.03024 on 1 and 50081 DF,  p-value: 0.862\n\n# Optional: mean difference (difference in proportions)\nmean_diff &lt;- with(df, mean(dormant[treatment == 1]) - mean(dormant[treatment == 0]))\ncat(\"Difference in proportion dormant (Treatment - Control):\", round(mean_diff, 3), \"\\n\")\n\nDifference in proportion dormant (Treatment - Control): 0.001 \n\n\n\n\n\n\nBalance Table for Key Pre-Treatment Variables\n\n\n\nControl Mean (SD)\nTreatment Mean (SD)\np-value (t-test)\n\n\n\n\nmrm2\n13.00 (12.07)\n13.01 (12.09)\n0.905\n\n\nfreq\n8.05 (11.40)\n8.04 (11.39)\n0.912\n\n\ndormant\n0.52 (0.50)\n0.52 (0.50)\n0.862\n\n\n\n\nComment on Balance Tests\nTo validate the random assignment process in this experiment, we conducted balance checks on three key pre-treatment variables: freq (donation frequency), mrm2 (months since last donation), and dormant (inactivity indicator). These variables represent donor history and engagement prior to treatment.\nUsing both Welch’s two-sample t-tests and simple linear regression models (variable ~ treatment), we found that none of the variables showed statistically significant differences between the treatment and control groups at the 95% confidence level. As expected, the t-statistics and p-values from both methods were consistent, confirming that the linear regression approach reproduces the t-test results.\nThese findings mirror Table 1 in Karlan & List (2007), where the authors show that their randomized assignment procedure resulted in balanced groups across observable characteristics and donation history. Including such a table helps reassure readers that observed treatment effects are not driven by underlying differences between groups. As in the original paper, our results suggest that the experimental design successfully isolated the causal impact of the match offer on donation behavior."
  },
  {
    "objectID": "blog/project3/index.html#experimental-results",
    "href": "blog/project3/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\nProportion of Donors\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndonate_summary &lt;- data %&gt;%\n  filter(!is.na(treatment), !is.na(gave)) %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(prop_donated = mean(gave)) %&gt;%\n  mutate(group = factor(ifelse(treatment == 1, \"Treatment\", \"Control\"), \n                        levels = c(\"Control\", \"Treatment\")))\n\nggplot(donate_summary, aes(x = group, y = prop_donated)) +\n  geom_col(fill = \"#A9C5D3\", width = 0.4) +\n  geom_text(aes(label = scales::percent(prop_donated, accuracy = 0.1)),\n            vjust = -0.5, size = 4.5) +\n  scale_y_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    limits = c(0, max(donate_summary$prop_donated) + 0.05)\n  ) +\n  labs(\n    title = \"Proportion of Donors by Group\",\n    x = NULL,\n    y = \"Donation Rate\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    axis.text.x = element_text(size = 12),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\n\n\n\n\nProportion of Donors in Treatment vs Control Groups\n\n\n\n\n\n\n\nTreatment Effect on Donation Behavior\n\n\nShow code for t-test and regression\n# Filter to non-missing values\ndf &lt;- data %&gt;% filter(!is.na(treatment), !is.na(gave))\n\n# Run t-test\nt_result &lt;- t.test(gave ~ treatment, data = df)\n\n# Run linear regression\nreg_result &lt;- lm(gave ~ treatment, data = df)\n\n# Show results\ncat(\"### T-test Result:\\n\")\n\n\n### T-test Result:\n\n\nShow code for t-test and regression\nprint(t_result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n\nShow code for t-test and regression\ncat(\"\\n### Linear Regression Result:\\n\")\n\n\n\n### Linear Regression Result:\n\n\nShow code for t-test and regression\nsummary(reg_result)\n\n\n\nCall:\nlm(formula = gave ~ treatment, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02204 -0.02204 -0.02204 -0.01786  0.98214 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.017858   0.001101  16.225  &lt; 2e-16 ***\ntreatment   0.004180   0.001348   3.101  0.00193 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1422 on 50081 degrees of freedom\nMultiple R-squared:  0.000192,  Adjusted R-squared:  0.0001721 \nF-statistic: 9.618 on 1 and 50081 DF,  p-value: 0.001927\n\n\n\n\n\n\n\n\nWe conducted a t-test to compare the proportion of donors between the treatment group (who were offered a matching donation) and the control group. The test revealed a statistically significant difference: those offered the match were more likely to donate.\nA simple regression of donation behavior (gave) on the treatment indicator confirmed the same result — the treatment increased the likelihood of giving. The magnitude of this effect is modest in absolute terms, but statistically significant at the 5% level.\nIn plain language: simply mentioning a match offer nudged more people to give. This aligns with the findings in Table 2A, Panel A of the original paper. The result highlights a key insight about human behavior: subtle psychological cues, like a match from a “concerned fellow member,” can meaningfully influence charitable action — even without changing the actual economic payoff in a big way.\n\n\n\n\n\n\nProbit Model of Donation Behavior\n\n\nShow code for probit model\nlibrary(dplyr)\n\n# Use the probit family in glm\ndf &lt;- data %&gt;% filter(!is.na(gave), !is.na(treatment))\n\n# Run probit regression\nprobit_model &lt;- glm(gave ~ treatment, family = binomial(link = \"probit\"), data = df)\n\n# Show results\nsummary(probit_model)\n\n\n\nCall:\nglm(formula = gave ~ treatment, family = binomial(link = \"probit\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.10014    0.02332 -90.074  &lt; 2e-16 ***\ntreatment    0.08678    0.02788   3.113  0.00185 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10071  on 50082  degrees of freedom\nResidual deviance: 10061  on 50081  degrees of freedom\nAIC: 10065\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\nWe ran a probit regression to test whether being offered a matching donation increased the likelihood of giving. The treatment effect was positive and statistically significant, confirming the result in Table 3, Column 1 of Karlan & List (2007).\nThis suggests that simply mentioning a match offer made people more likely to donate.\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n\nTesting the Effect of Match Size on Donations\n\n\nShow code for match ratio comparisons\nlibrary(dplyr)\n\n# Filter for treatment group only (not control) and non-missing values\ndf_match &lt;- data %&gt;%\n  filter(!is.na(gave), treatment == 1, !is.na(ratio))\n\n# T-test: 2:1 vs 1:1\nt_2v1 &lt;- t.test(gave ~ ratio, data = df_match %&gt;% filter(ratio %in% c(1, 2)))\n\n# T-test: 3:1 vs 1:1\nt_3v1 &lt;- t.test(gave ~ ratio, data = df_match %&gt;% filter(ratio %in% c(1, 3)))\n\n# T-test: 3:1 vs 2:1\nt_3v2 &lt;- t.test(gave ~ ratio, data = df_match %&gt;% filter(ratio %in% c(2, 3)))\n\n# Print results\ncat(\"### T-test: 2:1 vs 1:1\\n\")\n\n\n### T-test: 2:1 vs 1:1\n\n\nShow code for match ratio comparisons\nprint(t_2v1)\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.005711275  0.001942773\nsample estimates:\nmean in group 1 mean in group 2 \n     0.02074912      0.02263338 \n\n\nShow code for match ratio comparisons\ncat(\"\\n### T-test: 3:1 vs 1:1\\n\")\n\n\n\n### T-test: 3:1 vs 1:1\n\n\nShow code for match ratio comparisons\nprint(t_3v1)\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.005816051  0.001847501\nsample estimates:\nmean in group 1 mean in group 3 \n     0.02074912      0.02273340 \n\n\nShow code for match ratio comparisons\ncat(\"\\n### T-test: 3:1 vs 2:1\\n\")\n\n\n\n### T-test: 3:1 vs 2:1\n\n\nShow code for match ratio comparisons\nprint(t_3v2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  gave by ratio\nt = -0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means between group 2 and group 3 is not equal to 0\n95 percent confidence interval:\n -0.004012044  0.003811996\nsample estimates:\nmean in group 2 mean in group 3 \n     0.02263338      0.02273340 \n\n\n\n\n\n\n\n\nWe tested whether higher match ratios (2:1 or 3:1) led to higher donation rates than the standard 1:1 match. None of the pairwise comparisons showed a statistically significant difference.\nThis supports the authors’ comment on page 8 of the paper that “larger match ratios […] have no additional impact.” In other words, simply offering some match is what drives behavior — increasing the match size doesn’t further increase donations.\n\n\n\n\n\n\nRegression: Effect of Match Ratio on Giving\n\n\nShow regression by match ratio\nlibrary(dplyr)\n\n# Use only treatment group and valid ratio\ndf_match &lt;- data %&gt;% filter(treatment == 1, !is.na(ratio), !is.na(gave))\n\n# Create dummy variables for each ratio\ndf_match &lt;- df_match %&gt;%\n  mutate(\n    ratio1 = as.integer(ratio == 1),\n    ratio2 = as.integer(ratio == 2),\n    ratio3 = as.integer(ratio == 3)\n  )\n\n# Run regression using ratio1 as reference\nlm_ratio_dummies &lt;- lm(gave ~ ratio2 + ratio3, data = df_match)\nsummary(lm_ratio_dummies)\n\n\n\nCall:\nlm(formula = gave ~ ratio2 + ratio3, data = df_match)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.020749   0.001391  14.912   &lt;2e-16 ***\nratio2      0.001884   0.001968   0.958    0.338    \nratio3      0.001984   0.001968   1.008    0.313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\nShow regression by match ratio\n# Alternative: use ratio as a factor (automatically uses 1:1 as reference)\ndf_match$ratio &lt;- factor(df_match$ratio)\nlm_ratio_factor &lt;- lm(gave ~ ratio, data = df_match)\nsummary(lm_ratio_factor)\n\n\n\nCall:\nlm(formula = gave ~ ratio, data = df_match)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.02273 -0.02273 -0.02263 -0.02075  0.97925 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.020749   0.001391  14.912   &lt;2e-16 ***\nratio2      0.001884   0.001968   0.958    0.338    \nratio3      0.001984   0.001968   1.008    0.313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1468 on 33393 degrees of freedom\nMultiple R-squared:  3.865e-05, Adjusted R-squared:  -2.124e-05 \nF-statistic: 0.6454 on 2 and 33393 DF,  p-value: 0.5245\n\n\n\n\n\n\n\n\nWe regressed donation behavior on match ratio levels, using 1:1 as the reference group. The coefficients on the 2:1 and 3:1 match ratios were small and not statistically significant, suggesting that higher match ratios did not meaningfully increase the likelihood of giving.\nThese results support the finding in the paper that simply having a match matters — but increasing the size of the match doesn’t make people more likely to donate.\n\n\n\n\n\n\nDifference in Response Rates Between Match Ratios\n\n\nShow calculations for response rate differences\nlibrary(dplyr)\n\n# Filter treatment group only\ndf_match &lt;- data %&gt;%\n  filter(treatment == 1, !is.na(ratio), !is.na(gave)) %&gt;%\n  mutate(ratio = factor(ratio))\n\n# 1. Direct calculation from data\ngroup_means &lt;- df_match %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave)) %&gt;%\n  arrange(ratio)\n\n# Extract response rate differences\ndiff_2v1_data &lt;- group_means$response_rate[group_means$ratio == 2] - \n                 group_means$response_rate[group_means$ratio == 1]\n\ndiff_3v2_data &lt;- group_means$response_rate[group_means$ratio == 3] - \n                 group_means$response_rate[group_means$ratio == 2]\n\n# 2. From regression coefficients\nlm_ratio &lt;- lm(gave ~ ratio, data = df_match)\ncoefs &lt;- coef(lm_ratio)\n\n# ratio2 vs ratio1 (reference): coefficient of ratio2\ndiff_2v1_model &lt;- coefs[\"ratio2\"]\n\n# ratio3 vs ratio2: difference of coefficients\ndiff_3v2_model &lt;- coefs[\"ratio3\"] - coefs[\"ratio2\"]\n\n# Display results\ncat(\"### From Raw Data:\\n\")\n\n\n### From Raw Data:\n\n\nShow calculations for response rate differences\ncat(\"Difference in response rate (2:1 - 1:1):\", round(diff_2v1_data, 4), \"\\n\")\n\n\nDifference in response rate (2:1 - 1:1): 0.0019 \n\n\nShow calculations for response rate differences\ncat(\"Difference in response rate (3:1 - 2:1):\", round(diff_3v2_data, 4), \"\\n\\n\")\n\n\nDifference in response rate (3:1 - 2:1): 1e-04 \n\n\nShow calculations for response rate differences\ncat(\"### From Regression Coefficients:\\n\")\n\n\n### From Regression Coefficients:\n\n\nShow calculations for response rate differences\ncat(\"Difference in response rate (2:1 - 1:1):\", round(diff_2v1_model, 4), \"\\n\")\n\n\nDifference in response rate (2:1 - 1:1): 0.0019 \n\n\nShow calculations for response rate differences\ncat(\"Difference in response rate (3:1 - 2:1):\", round(diff_3v2_model, 4), \"\\n\")\n\n\nDifference in response rate (3:1 - 2:1): 1e-04 \n\n\n\n\n\n\n\n\nWe compared donation rates across match ratios using both raw data and regression coefficients. The difference in response rates between the 1:1 and 2:1 match was small, and the difference between 2:1 and 3:1 was even smaller — and neither was statistically significant. This supports the conclusion from the paper that increasing the match ratio doesn’t lead to meaningful increases in donation likelihood. The presence of a match matters; its size does not.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nEffect of Treatment on Donation Amount\n\n\nShow code for amount vs treatment analysis\nlibrary(dplyr)\n\n# Filter to non-missing donation amount and treatment\ndf_amount &lt;- data %&gt;% filter(!is.na(amount), !is.na(treatment))\n\n# T-test\nt_amount &lt;- t.test(amount ~ treatment, data = df_amount)\n\n# Linear regression\nlm_amount &lt;- lm(amount ~ treatment, data = df_amount)\n\n# Show results\ncat(\"### T-test: Donation Amount by Treatment Group\\n\")\n\n\n### T-test: Donation Amount by Treatment Group\n\n\nShow code for amount vs treatment analysis\nprint(t_amount)\n\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = -1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.310555423  0.003344493\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8132678       0.9668733 \n\n\nShow code for amount vs treatment analysis\ncat(\"\\n### Linear Regression: amount ~ treatment\\n\")\n\n\n\n### Linear Regression: amount ~ treatment\n\n\nShow code for amount vs treatment analysis\nsummary(lm_amount)\n\n\n\nCall:\nlm(formula = amount ~ treatment, data = df_amount)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.97  -0.97  -0.97  -0.81 399.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.81327    0.06742  12.063   &lt;2e-16 ***\ntreatment    0.15361    0.08256   1.861   0.0628 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.709 on 50081 degrees of freedom\nMultiple R-squared:  6.911e-05, Adjusted R-squared:  4.915e-05 \nF-statistic: 3.461 on 1 and 50081 DF,  p-value: 0.06282\n\n\n\n\n\n\n\n\nWe tested whether the treatment — being offered a matching donation — affected the amount donated, not just the decision to donate. The difference in average donation amounts between treatment and control groups was small and statistically insignificant.\nThis mirrors the findings in the original paper: the treatment increased the probability of giving but didn’t change how much donors gave. In other words, the match offer works primarily by encouraging more people to donate, not by increasing donation sizes among those who already planned to give.\n\n\n\n\n\n\nEffect of Treatment on Donation Amount (Conditional on Donating)\n\n\nShow code for conditional donation regression\nlibrary(dplyr)\n\n# Filter to donors only\ndf_donors &lt;- data %&gt;% filter(!is.na(amount), !is.na(treatment), amount &gt; 0)\n\n# Run regression on donation amount (only for donors)\nlm_donors &lt;- lm(amount ~ treatment, data = df_donors)\n\n# Show results\nsummary(lm_donors)\n\n\n\nCall:\nlm(formula = amount ~ treatment, data = df_donors)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-43.54 -23.87 -18.87   6.13 356.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.540      2.423  18.792   &lt;2e-16 ***\ntreatment     -1.668      2.872  -0.581    0.561    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.83 on 1032 degrees of freedom\nMultiple R-squared:  0.0003268, Adjusted R-squared:  -0.0006419 \nF-statistic: 0.3374 on 1 and 1032 DF,  p-value: 0.5615\n\n\n\n\n\n\n\n\nWe limited the analysis to donors only and ran a regression of donation amount on treatment status. The treatment coefficient captures whether match messaging affects how much people give, conditional on giving.\nThe coefficient was small and not statistically significant, suggesting the treatment did not increase donation amounts among those who chose to donate.\nThis result does not have a clean causal interpretation because the analysis excludes non-donors — the treatment is no longer randomly assigned in this subset.\n\n\n\n\n\n\n\nDonation Amounts by Group (Only Donors)\n\n\nShow code for donation histograms with styled title\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Step 1: Keep only donors (positive amounts)\ndf_donors &lt;- data %&gt;%\n  filter(!is.na(amount), amount &gt; 0, !is.na(treatment))\n\n# Step 2: Calculate the 99th percentile cutoff to trim outliers\ncutoff &lt;- quantile(df_donors$amount, 0.99)\n\n# Step 3: Filter the dataset for plotting (but keep full data for analysis)\ndf_trimmed &lt;- df_donors %&gt;% filter(amount &lt;= cutoff)\n\n# Step 4: Create the histogram with group means\nggplot(df_trimmed, aes(x = amount)) +\n  geom_histogram(binwidth = 5, fill = \"#A9C5D3\", color = \"white\") +\n  geom_vline(\n    data = df_trimmed %&gt;% group_by(treatment) %&gt;% summarise(avg = mean(amount)),\n    aes(xintercept = avg),\n    color = \"red\", linetype = \"dashed\", linewidth = 1\n  ) +\n  facet_wrap(~ treatment, labeller = as_labeller(c(`0` = \"Control Group\", `1` = \"Treatment Group\"))) +\n  labs(\n    x = \"Donation Amount (Trimmed at 99th Percentile)\",\n    y = \"Number of Donors\",\n    title = \"Histogram of Donation Amounts (Among Donors)\",\n    subtitle = \"Red dashed line shows group average\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nHistogram of Donation Amounts (Trimmed at 99th Percentile)\n\n\n\n\n\n\nSimulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\n\n\n\nSimulating the Treatment Effect (Cumulative Average Plot)\n\n\nShow code for cumulative average treatment effect plot\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Filter to donors only\ndf_donors &lt;- data %&gt;%\n  filter(!is.na(amount), amount &gt; 0, !is.na(treatment))\n\n# Separate control and treatment distributions\ncontrol_amounts &lt;- df_donors %&gt;% filter(treatment == 0) %&gt;% pull(amount)\ntreat_amounts   &lt;- df_donors %&gt;% filter(treatment == 1) %&gt;% pull(amount)\n\n# Simulate draws\nset.seed(123)\nsim_control &lt;- sample(control_amounts, size = 100000, replace = TRUE)\nsim_treat   &lt;- sample(treat_amounts,   size = 10000,  replace = TRUE)\n\n# Compute differences and cumulative average\nsim_diff &lt;- sim_treat - sim_control[1:10000]\ncum_avg &lt;- cumsum(sim_diff) / seq_along(sim_diff)\n\n# Calculate true difference in means\ntrue_diff &lt;- mean(treat_amounts) - mean(control_amounts)\n\n# Plot\nggplot(data.frame(draw = 1:10000, cum_avg = cum_avg), aes(x = draw, y = cum_avg)) +\n  geom_line(color = \"#A9C5D3\", linewidth = 1) +\n  geom_hline(yintercept = true_diff, linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray70\") +\n  labs(\n    x = \"Simulation Draw\",\n    y = \"Cumulative Average Difference\",\n    title = \"Cumulative Average of Treatment - Control (Simulated)\",\n    subtitle = \"Dashed red line = true difference, gray line = 0 reference\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\nCumulative Average of Simulated Treatment-Control Differences\n\n\n\n\n\n\n\n\n\n\nThis plot shows the cumulative average difference in donation amounts between simulated treatment and control draws. The dashed red line represents the true average treatment effect (mean difference), while the solid gray line marks zero.\nAs we simulate more draws, the cumulative average quickly stabilizes near the true difference. This illustrates the law of large numbers — as the number of observations grows, the average of our simulated differences converges to the actual effect. It’s a visual reminder that random variation in small samples can mislead, but with large enough data, we can trust the estimated treatment effect.\n\n\n\n\n\nCentral Limit Theorem\n\n\nSampling Distribution of Average Treatment Effects\n\n\nShow code for histograms at different sample sizes\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Use donor data only\ndf_donors &lt;- data %&gt;% filter(!is.na(amount), amount &gt; 0, !is.na(treatment))\ncontrol_vals &lt;- df_donors %&gt;% filter(treatment == 0) %&gt;% pull(amount)\ntreat_vals   &lt;- df_donors %&gt;% filter(treatment == 1) %&gt;% pull(amount)\n\nsimulate_differences &lt;- function(n, reps = 1000) {\n  replicate(reps, {\n    mean(sample(treat_vals, n, replace = TRUE)) - mean(sample(control_vals, n, replace = TRUE))\n  })\n}\n\n# Simulate for each sample size\nsizes &lt;- c(50, 200, 500, 1000)\nsim_results &lt;- lapply(sizes, simulate_differences)\nnames(sim_results) &lt;- paste0(\"n = \", sizes)\n\n# Combine into a single data frame for faceting\nsim_df &lt;- do.call(rbind, lapply(names(sim_results), function(name) {\n  data.frame(\n    diff = sim_results[[name]],\n    size = name\n  )\n}))\n\n# Plot histograms\nggplot(sim_df, aes(x = diff)) +\n  geom_histogram(bins = 40, fill = \"#A9C5D3\", color = \"white\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  facet_wrap(~ size, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Sampling Distributions of Estimated Treatment Effects\",\n    subtitle = \"Each histogram is based on 1,000 simulations at different sample sizes\",\n    x = \"Estimated Treatment Effect\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nSampling Distributions of Estimated Treatment Effects at Varying Sample Sizes\n\n\n\n\n\n\n\n\n\n\nEach histogram shows the distribution of estimated treatment effects from 1,000 simulations at different sample sizes. As the sample size increases (from 50 to 1,000), the distribution of estimated effects becomes narrower and more centered around the true effect.\nWhen the sample size is small (e.g., 50), the distribution is wide and zero is near the middle, suggesting high uncertainty — we’re just as likely to estimate an effect above or below zero. But as the sample size grows, the estimates converge and the chance of zero falling near the center shrinks. By n = 1000, zero is clearly in the tail of the distribution, suggesting we’re more confident the true effect is positive.\nThis demonstrates why larger samples increase statistical power and give us more reliable estimates."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sanjit Kangovi",
    "section": "",
    "text": "I’m passionate about using data and technology to solve real-world business problems. Throughout my journey, I’ve been fortunate to work on projects that span multiple industries—each experience teaching me something new and helping me grow.\nDuring my time as a Power BI Developer, I had the opportunity to work with clients in finance, logistics, and other fields, building customized dashboards that made data more accessible and decision-making more informed. It was exciting to see how data could directly impact business outcomes, and I enjoyed working closely with clients to make that happen.\nAs a Co-founder of Minics Digital, I dove into the world of digital marketing and client management. I worked with small businesses to craft personalized strategies that helped them build their online presence. Leading multiple teams and managing projects end-to-end was challenging, but watching these startups grow made it all worth it.\nRight now, I’m eager to take on new opportunities where I can combine my data analytics skills with my passion for client-focused problem-solving. I’m always looking forward to the next challenge!"
  }
]