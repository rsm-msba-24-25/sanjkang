---
title: "A Replication of Karlan and List (2007)"
author: "Sanjit Kangovi"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

<!-- _to do: expand on the description of the experiment._ -->

This project seeks to replicate their results.


## Data

```{r}
#| echo: true
#| results: 'asis'
#| message: false
#| warning: false

library(haven)
library(dplyr)
data <- read_dta("/Users/sanjitkangovi/Desktop/mysite/blog/project3/karlan_list_2007.dta")
```

### Description

<!-- #_todo: Read the data into R/Python and describe the data_ -->


```{r}
#| echo: false
library(haven)
library(dplyr)
library(knitr)
library(tibble)

data <- read_dta("/Users/sanjitkangovi/Desktop/mysite/blog/project3/karlan_list_2007.dta")
```

:::: {.callout-note collapse="true"}
### Structure of the Dataset

```{r}
glimpse(data)
```
::::

:::: {.callout-note collapse="true"}
### Summary of Numeric Variables

```{r}
summary_df <- as.data.frame(summary(select(data, where(is.numeric))))
kable(summary_df, caption = "Summary Statistics")
```
::::

:::: {.callout-note collapse="true"}
### Missing Values

```{r}
missing_df <- data.frame(
  Variable = names(data),
  Missing = colSums(is.na(data))
)
kable(missing_df, caption = "Missing Values Per Variable")
```
::::

:::: {.callout-note collapse="true"}
### Summary by Treatment Group

```{r}
grouped_summary <- data %>%
  group_by(treatment) %>%
  summarise(
    response_rate = mean(gave, na.rm = TRUE),
    avg_donation = mean(amount, na.rm = TRUE),
    n = n()
  )
kable(grouped_summary, caption = "Summary by Treatment Group")
```
::::





:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

<!-- _todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._ -->

:::: {.callout-note collapse="true"}
### Balance Test: `mrm2` (Months Since Last Donation)

```{r}
# Subset to remove missing values
df <- data %>% filter(!is.na(mrm2), !is.na(treatment))

# T-test using base R (Welch’s t-test, unequal variances)
t_test_result <- t.test(mrm2 ~ treatment, data = df)
print(t_test_result)

# Linear regression: mrm2 ~ treatment
reg_result <- lm(mrm2 ~ treatment, data = df)
summary(reg_result)

# Mean difference directly
mean_diff <- with(df, mean(mrm2[treatment == 1]) - mean(mrm2[treatment == 0]))
cat("Mean difference (Treatment - Control):", round(mean_diff, 3), "\n")
```
::::

:::: {.callout-note collapse="true"}
### Balance Test: `freq` (Donation Frequency)

```{r}
# Subset data to remove missing values for freq and treatment
df <- data %>% filter(!is.na(freq), !is.na(treatment))

# T-test (Welch's two-sample t-test for unequal variances)
t_test_freq <- t.test(freq ~ treatment, data = df)
print(t_test_freq)

# Linear regression: freq ~ treatment
lm_freq <- lm(freq ~ treatment, data = df)
summary(lm_freq)

# Optional: manually calculate mean difference
mean_diff <- with(df, mean(freq[treatment == 1]) - mean(freq[treatment == 0]))
cat("Mean difference (Treatment - Control):", round(mean_diff, 3), "\n")
```
::::

:::: {.callout-note collapse="true"}
### Balance Test: `dormant` (Inactivity Indicator)

```{r}
# Filter out missing values
df <- data %>% filter(!is.na(dormant), !is.na(treatment))

# T-test (Welch's test for difference in proportions since dormant is binary)
t_test_dormant <- t.test(dormant ~ treatment, data = df)
print(t_test_dormant)

# Linear regression: dormant ~ treatment
lm_dormant <- lm(dormant ~ treatment, data = df)
summary(lm_dormant)

# Optional: mean difference (difference in proportions)
mean_diff <- with(df, mean(dormant[treatment == 1]) - mean(dormant[treatment == 0]))
cat("Difference in proportion dormant (Treatment - Control):", round(mean_diff, 3), "\n")
```
::::
```{r}
#| echo: false
#| results: 'asis'
#| message: false
#| warning: false

library(dplyr)
library(knitr)

# Variables to include in the balance table
vars <- c("mrm2", "freq", "dormant")

# Balance function
balance_summary <- function(var, data) {
  df <- data %>% filter(!is.na(.data[[var]]), !is.na(treatment))
  t_res <- t.test(df[[var]] ~ df$treatment)
  
  control <- df %>% filter(treatment == 0) %>% pull(var)
  treatment <- df %>% filter(treatment == 1) %>% pull(var)
  
  c(
    sprintf("%.2f (%.2f)", mean(control), sd(control)),
    sprintf("%.2f (%.2f)", mean(treatment), sd(treatment)),
    sprintf("%.3f", t_res$p.value)
  )
}

# Create the summary table
results <- t(sapply(vars, balance_summary, data = data))
colnames(results) <- c("Control Mean (SD)", "Treatment Mean (SD)", "p-value (t-test)")
rownames(results) <- vars

# Show the table
kable(results, caption = "Balance Table for Key Pre-Treatment Variables")
```


#### **Comment on Balance Tests**

To validate the random assignment process in this experiment, we conducted balance checks on three key pre-treatment variables: `freq` (donation frequency), `mrm2` (months since last donation), and `dormant` (inactivity indicator). These variables represent donor history and engagement prior to treatment.

Using both Welch's two-sample t-tests and simple linear regression models (`variable ~ treatment`), we found that none of the variables showed statistically significant differences between the treatment and control groups at the 95% confidence level. As expected, the t-statistics and p-values from both methods were consistent, confirming that the linear regression approach reproduces the t-test results.

These findings mirror **Table 1** in *Karlan & List (2007)*, where the authors show that their randomized assignment procedure resulted in balanced groups across observable characteristics and donation history. Including such a table helps reassure readers that observed treatment effects are not driven by underlying differences between groups. As in the original paper, our results suggest that the experimental design successfully isolated the causal impact of the match offer on donation behavior.

## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

<!-- _todo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control._ -->
### Proportion of Donors {#donation-barplot}

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| fig-cap: "Proportion of Donors in Treatment vs Control Groups"
#| fig-width: 6
#| fig-height: 5
#| message: false
#| warning: false

library(dplyr)
library(ggplot2)

donate_summary <- data %>%
  filter(!is.na(treatment), !is.na(gave)) %>%
  group_by(treatment) %>%
  summarise(prop_donated = mean(gave)) %>%
  mutate(group = factor(ifelse(treatment == 1, "Treatment", "Control"), 
                        levels = c("Control", "Treatment")))

ggplot(donate_summary, aes(x = group, y = prop_donated)) +
  geom_col(fill = "#A9C5D3", width = 0.4) +
  geom_text(aes(label = scales::percent(prop_donated, accuracy = 0.1)),
            vjust = -0.5, size = 4.5) +
  scale_y_continuous(
    labels = scales::percent_format(accuracy = 1),
    limits = c(0, max(donate_summary$prop_donated) + 0.05)
  ) +
  labs(
    title = "Proportion of Donors by Group",
    x = NULL,
    y = "Donation Rate"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.text.x = element_text(size = 12),
    axis.title.y = element_text(margin = margin(r = 10))
  )
```




<!-- _todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)_ -->

### Treatment Effect on Donation Behavior

```{r}
#| code-fold: true
#| code-summary: "Show code for t-test and regression"
#| message: false
#| warning: false

# Filter to non-missing values
df <- data %>% filter(!is.na(treatment), !is.na(gave))

# Run t-test
t_result <- t.test(gave ~ treatment, data = df)

# Run linear regression
reg_result <- lm(gave ~ treatment, data = df)

# Show results
cat("### T-test Result:\n")
print(t_result)

cat("\n### Linear Regression Result:\n")
summary(reg_result)
```

::: {.callout-note}
We conducted a **t-test** to compare the proportion of donors between the **treatment group** (who were offered a matching donation) and the **control group**. The test revealed a **statistically significant difference**: those offered the match were **more likely to donate**.

A simple regression of donation behavior (`gave`) on the treatment indicator confirmed the same result — the **treatment increased the likelihood of giving**. The magnitude of this effect is **modest in absolute terms**, but **statistically significant at the 5% level**.

In plain language: simply **mentioning a match offer nudged more people to give**. This aligns with the findings in **Table 2A, Panel A** of the original paper. The result highlights a key insight about **human behavior**: subtle **psychological cues**, like a match from a “concerned fellow member,” can **meaningfully influence charitable action** — even without changing the actual economic payoff in a big way.
:::


<!-- _todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._ -->

### Probit Model of Donation Behavior

```{r}
#| code-fold: true
#| code-summary: "Show code for probit model"
#| message: false
#| warning: false

library(dplyr)

# Use the probit family in glm
df <- data %>% filter(!is.na(gave), !is.na(treatment))

# Run probit regression
probit_model <- glm(gave ~ treatment, family = binomial(link = "probit"), data = df)

# Show results
summary(probit_model)
```

::: {.callout-note}

We ran a **probit regression** to test whether being offered a **matching donation** increased the **likelihood of giving**. The **treatment effect was positive and statistically significant**, confirming the result in **Table 3, Column 1** of *Karlan & List (2007)*.

This suggests that simply **mentioning a match offer** made people **more likely to donate**.
:::


### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

<!-- _todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the "figures suggest" comment the authors make on page 8?_ -->

### Testing the Effect of Match Size on Donations

```{r}
#| code-fold: true
#| code-summary: "Show code for match ratio comparisons"
#| message: false
#| warning: false

library(dplyr)

# Filter for treatment group only (not control) and non-missing values
df_match <- data %>%
  filter(!is.na(gave), treatment == 1, !is.na(ratio))

# T-test: 2:1 vs 1:1
t_2v1 <- t.test(gave ~ ratio, data = df_match %>% filter(ratio %in% c(1, 2)))

# T-test: 3:1 vs 1:1
t_3v1 <- t.test(gave ~ ratio, data = df_match %>% filter(ratio %in% c(1, 3)))

# T-test: 3:1 vs 2:1
t_3v2 <- t.test(gave ~ ratio, data = df_match %>% filter(ratio %in% c(2, 3)))

# Print results
cat("### T-test: 2:1 vs 1:1\n")
print(t_2v1)

cat("\n### T-test: 3:1 vs 1:1\n")
print(t_3v1)

cat("\n### T-test: 3:1 vs 2:1\n")
print(t_3v2)
```

::: {.callout-note}

We tested whether **higher match ratios (2:1 or 3:1)** led to higher donation rates than the standard **1:1 match**. None of the **pairwise comparisons** showed a **statistically significant difference**. 

This supports the authors’ comment on **page 8** of the paper that “larger match ratios […] have no additional impact.” In other words, simply **offering some match** is what drives behavior — increasing the **match size** doesn't further increase donations.
:::

<!-- _todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._ -->

### Regression: Effect of Match Ratio on Giving

```{r}
#| code-fold: true
#| code-summary: "Show regression by match ratio"
#| message: false
#| warning: false

library(dplyr)

# Use only treatment group and valid ratio
df_match <- data %>% filter(treatment == 1, !is.na(ratio), !is.na(gave))

# Create dummy variables for each ratio
df_match <- df_match %>%
  mutate(
    ratio1 = as.integer(ratio == 1),
    ratio2 = as.integer(ratio == 2),
    ratio3 = as.integer(ratio == 3)
  )

# Run regression using ratio1 as reference
lm_ratio_dummies <- lm(gave ~ ratio2 + ratio3, data = df_match)
summary(lm_ratio_dummies)

# Alternative: use ratio as a factor (automatically uses 1:1 as reference)
df_match$ratio <- factor(df_match$ratio)
lm_ratio_factor <- lm(gave ~ ratio, data = df_match)
summary(lm_ratio_factor)
```
::: {.callout-note}

We regressed donation behavior on match ratio levels, using **1:1 as the reference group**. The coefficients on the **2:1 and 3:1 match ratios** were **small and not statistically significant**, suggesting that higher match ratios did not meaningfully increase the likelihood of giving.

These results support the finding in the paper that simply **having a match** matters — but **increasing the size** of the match doesn’t make people more likely to donate.
:::



<!-- _todo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.  Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_ -->

### Difference in Response Rates Between Match Ratios

```{r}
#| code-fold: true
#| code-summary: "Show calculations for response rate differences"
#| message: false
#| warning: false

library(dplyr)

# Filter treatment group only
df_match <- data %>%
  filter(treatment == 1, !is.na(ratio), !is.na(gave)) %>%
  mutate(ratio = factor(ratio))

# 1. Direct calculation from data
group_means <- df_match %>%
  group_by(ratio) %>%
  summarise(response_rate = mean(gave)) %>%
  arrange(ratio)

# Extract response rate differences
diff_2v1_data <- group_means$response_rate[group_means$ratio == 2] - 
                 group_means$response_rate[group_means$ratio == 1]

diff_3v2_data <- group_means$response_rate[group_means$ratio == 3] - 
                 group_means$response_rate[group_means$ratio == 2]

# 2. From regression coefficients
lm_ratio <- lm(gave ~ ratio, data = df_match)
coefs <- coef(lm_ratio)

# ratio2 vs ratio1 (reference): coefficient of ratio2
diff_2v1_model <- coefs["ratio2"]

# ratio3 vs ratio2: difference of coefficients
diff_3v2_model <- coefs["ratio3"] - coefs["ratio2"]

# Display results
cat("### From Raw Data:\n")
cat("Difference in response rate (2:1 - 1:1):", round(diff_2v1_data, 4), "\n")
cat("Difference in response rate (3:1 - 2:1):", round(diff_3v2_data, 4), "\n\n")

cat("### From Regression Coefficients:\n")
cat("Difference in response rate (2:1 - 1:1):", round(diff_2v1_model, 4), "\n")
cat("Difference in response rate (3:1 - 2:1):", round(diff_3v2_model, 4), "\n")
```

::: {.callout-note}

We compared donation rates across match ratios using both raw data and regression coefficients. The difference in response rates between the 1:1 and 2:1 match was small, and the difference between 2:1 and 3:1 was even smaller — and neither was statistically significant. This supports the conclusion from the paper that increasing the match ratio doesn’t lead to meaningful increases in donation likelihood. The presence of a match matters; its size does not.
:::




### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

<!-- _todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_ -->

#### **Effect of Treatment on Donation Amount**

```{r}
#| code-fold: true
#| code-summary: "Show code for amount vs treatment analysis"
#| message: false
#| warning: false

library(dplyr)

# Filter to non-missing donation amount and treatment
df_amount <- data %>% filter(!is.na(amount), !is.na(treatment))

# T-test
t_amount <- t.test(amount ~ treatment, data = df_amount)

# Linear regression
lm_amount <- lm(amount ~ treatment, data = df_amount)

# Show results
cat("### T-test: Donation Amount by Treatment Group\n")
print(t_amount)

cat("\n### Linear Regression: amount ~ treatment\n")
summary(lm_amount)
```

::: {.callout-note}


We tested whether the treatment — being offered a matching donation — affected the **amount** donated, not just the decision to donate. The difference in average donation amounts between treatment and control groups was **small and statistically insignificant**.

This mirrors the findings in the original paper: the treatment increased the **probability** of giving but didn’t change **how much** donors gave. In other words, the match offer works primarily by encouraging more people to donate, not by increasing donation sizes among those who already planned to give.
:::


<!-- _todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_  -->

#### **Effect of Treatment on Donation Amount (Conditional on Donating)**

```{r}
#| code-fold: true
#| code-summary: "Show code for conditional donation regression"
#| message: false
#| warning: false

library(dplyr)

# Filter to donors only
df_donors <- data %>% filter(!is.na(amount), !is.na(treatment), amount > 0)

# Run regression on donation amount (only for donors)
lm_donors <- lm(amount ~ treatment, data = df_donors)

# Show results
summary(lm_donors)
```

::: {.callout-note}

We limited the analysis to donors only and ran a regression of donation amount on treatment status. The treatment coefficient captures whether match messaging affects how much people give, conditional on giving.

The coefficient was small and not statistically significant, suggesting the treatment did not increase donation amounts among those who chose to donate.

This result does **not** have a clean causal interpretation because the analysis excludes non-donors — the treatment is no longer randomly assigned in this subset.
:::


<!-- _todo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._ -->

### Donation Amounts by Group (Only Donors)

```{r}
#| code-fold: true
#| code-summary: "Show code for donation histograms with styled title"
#| fig-cap: "Histogram of Donation Amounts (Trimmed at 99th Percentile)"
#| fig-width: 10
#| fig-height: 5
#| message: false
#| warning: false

library(dplyr)
library(ggplot2)

# Step 1: Keep only donors (positive amounts)
df_donors <- data %>%
  filter(!is.na(amount), amount > 0, !is.na(treatment))

# Step 2: Calculate the 99th percentile cutoff to trim outliers
cutoff <- quantile(df_donors$amount, 0.99)

# Step 3: Filter the dataset for plotting (but keep full data for analysis)
df_trimmed <- df_donors %>% filter(amount <= cutoff)

# Step 4: Create the histogram with group means
ggplot(df_trimmed, aes(x = amount)) +
  geom_histogram(binwidth = 5, fill = "#A9C5D3", color = "white") +
  geom_vline(
    data = df_trimmed %>% group_by(treatment) %>% summarise(avg = mean(amount)),
    aes(xintercept = avg),
    color = "red", linetype = "dashed", linewidth = 1
  ) +
  facet_wrap(~ treatment, labeller = as_labeller(c(`0` = "Control Group", `1` = "Treatment Group"))) +
  labs(
    x = "Donation Amount (Trimmed at 99th Percentile)",
    y = "Number of Donors",
    title = "Histogram of Donation Amounts (Among Donors)",
    subtitle = "Red dashed line shows group average"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    strip.text = element_text(face = "bold")
  )
```





### Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

<!-- _to do:  Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._ -->

### Simulating the Treatment Effect (Cumulative Average Plot)


```{r}
#| code-fold: true
#| code-summary: "Show code for cumulative average treatment effect plot"
#| fig-cap: "Cumulative Average of Simulated Treatment-Control Differences"
#| fig-width: 8
#| fig-height: 5
#| message: false
#| warning: false

library(dplyr)
library(ggplot2)

# Filter to donors only
df_donors <- data %>%
  filter(!is.na(amount), amount > 0, !is.na(treatment))

# Separate control and treatment distributions
control_amounts <- df_donors %>% filter(treatment == 0) %>% pull(amount)
treat_amounts   <- df_donors %>% filter(treatment == 1) %>% pull(amount)

# Simulate draws
set.seed(123)
sim_control <- sample(control_amounts, size = 100000, replace = TRUE)
sim_treat   <- sample(treat_amounts,   size = 10000,  replace = TRUE)

# Compute differences and cumulative average
sim_diff <- sim_treat - sim_control[1:10000]
cum_avg <- cumsum(sim_diff) / seq_along(sim_diff)

# Calculate true difference in means
true_diff <- mean(treat_amounts) - mean(control_amounts)

# Plot
ggplot(data.frame(draw = 1:10000, cum_avg = cum_avg), aes(x = draw, y = cum_avg)) +
  geom_line(color = "#A9C5D3", linewidth = 1) +
  geom_hline(yintercept = true_diff, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0, linetype = "solid", color = "gray70") +
  labs(
    x = "Simulation Draw",
    y = "Cumulative Average Difference",
    title = "Cumulative Average of Treatment - Control (Simulated)",
    subtitle = "Dashed red line = true difference, gray line = 0 reference"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )
```

::: {.callout-note}

This plot shows the **cumulative average difference** in donation amounts between simulated treatment and control draws. The dashed red line represents the **true average treatment effect** (mean difference), while the solid gray line marks **zero**.

As we simulate more draws, the cumulative average quickly stabilizes near the true difference. This illustrates the **law of large numbers** — as the number of observations grows, the average of our simulated differences **converges** to the actual effect. It’s a visual reminder that random variation in small samples can mislead, but with large enough data, we can trust the estimated treatment effect.
:::

### Central Limit Theorem

<!-- _to do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the "middle" of the distribution or whether it's in the "tail."_ -->

#### **Sampling Distribution of Average Treatment Effects**

```{r}
#| code-fold: true
#| code-summary: "Show code for histograms at different sample sizes"
#| fig-cap: "Sampling Distributions of Estimated Treatment Effects at Varying Sample Sizes"
#| fig-width: 10
#| fig-height: 8
#| message: false
#| warning: false

library(dplyr)
library(ggplot2)

set.seed(123)

# Use donor data only
df_donors <- data %>% filter(!is.na(amount), amount > 0, !is.na(treatment))
control_vals <- df_donors %>% filter(treatment == 0) %>% pull(amount)
treat_vals   <- df_donors %>% filter(treatment == 1) %>% pull(amount)

simulate_differences <- function(n, reps = 1000) {
  replicate(reps, {
    mean(sample(treat_vals, n, replace = TRUE)) - mean(sample(control_vals, n, replace = TRUE))
  })
}

# Simulate for each sample size
sizes <- c(50, 200, 500, 1000)
sim_results <- lapply(sizes, simulate_differences)
names(sim_results) <- paste0("n = ", sizes)

# Combine into a single data frame for faceting
sim_df <- do.call(rbind, lapply(names(sim_results), function(name) {
  data.frame(
    diff = sim_results[[name]],
    size = name
  )
}))

# Plot histograms
ggplot(sim_df, aes(x = diff)) +
  geom_histogram(bins = 40, fill = "#A9C5D3", color = "white") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  facet_wrap(~ size, scales = "free", ncol = 2) +
  labs(
    title = "Sampling Distributions of Estimated Treatment Effects",
    subtitle = "Each histogram is based on 1,000 simulations at different sample sizes",
    x = "Estimated Treatment Effect",
    y = "Frequency"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    strip.text = element_text(face = "bold")
  )
```

::: {.callout-note}

Each histogram shows the distribution of estimated treatment effects from 1,000 simulations at different sample sizes. As the sample size increases (from 50 to 1,000), the distribution of estimated effects becomes **narrower and more centered** around the true effect.

When the sample size is small (e.g., 50), the distribution is **wide** and zero is near the **middle**, suggesting high uncertainty — we’re just as likely to estimate an effect above or below zero. But as the sample size grows, the estimates **converge** and the chance of zero falling near the center **shrinks**. By `n = 1000`, zero is clearly in the **tail** of the distribution, suggesting we’re more confident the true effect is positive.

This demonstrates why **larger samples increase statistical power** and give us **more reliable** estimates.